---
title: "Teaching a Robotic Arm to Move: Training AI in a Custom World"
publishedAt: "2025-01-25"
summary: "Training a robotic arm to move using AI in a custom environment"
tags: ["AI", "Robotics", "Machine Learning", "Reinforcement Learning"]
---

# Introduction

In my personal philosophy, I classify newly learned information into different tiers of truthness based on where it comes from.

Here's how it breaks down:

1. **Experienced it myself**Â â€“ The gold standard. If Iâ€™ve lived it, itâ€™s as true as it gets.
2. **Experienced by someone I know**Â â€“ Second-hand truth, but still reliable if the source is trustworthy.
3. **Anecdotes**Â â€“ The wild west of truth. Entertaining, but take it with a grain of salt.

Now Iâ€™ve heard great things about reinforcement learning. The promise of having an agent that knows nothing about its environment, learn how to operate in it through trial and error seems almost too good to be true!

 I find it to be a good model of how us humans operate. Think about it: on our 0th birthday, weâ€™re plopped into this vast, confusing universe. Through detecting patterns, experimenting, and learning from the consequences of our actions, each of us eventually become experts at living our own lives. (Admittedly, some of us are better at it than others.)

Iâ€™ve already dabbled with training AIs in pre-built environments, like the classic "balance a stick on a moving box" scenario. You know the one:

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313230302f312a475f7768744972593966476c773349743648466668412e676966.gif" 
      width={400} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>The Famous Environment: CartPole</figcaption>
  </div>
</figure>

But, being the skeptic that I am, I couldnâ€™t help but wonder:Â *Does RL really work, or did they just cherry-pick this one narrow task where it happens to perform well?*Â To put RL to the test, I decided to build my own custom environment from scratch and train an AI to operate in it.

# How to Build a Custom Environment So That Itâ€™s Ripe for Training

## But waitâ€¦ What are we building?

Letâ€™s start by dreaming up an ideaâ€”a universe for our little AI to spend its life in. Iâ€™ve always loved world-building in books/movies, and now itâ€™s time for me to create my own.

Okay, how about we build a **robot arm**? Itâ€™s a precursor to robots that can actually pick things up. To keep things simple, letâ€™s make it a 2D bot with 2D limbs.

We can make it do many things, like drawing, catching, juggling, waving, punching etc.

Now, technically, we donâ€™tÂ *have*Â to stick to human constraintsâ€”like making sure the elbow canâ€™t bend backwardâ€”but that would lookâ€¦ well, funky. I want to see human-like behavior emerge from this coded-up bot, so letâ€™s keep it somewhat realistic.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/20250124_135936.jpg" 
      width={280} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>First Draft: What a beauty</figcaption>
  </div>
</figure>

So Iâ€™m thinking itâ€™s gonna have two motors: one on the shoulder and another on the elbow. Weâ€™re just gonna ignore the fact that my bot doesnâ€™t have any hands.

*(Tryna simplify the environment so the AI learns faster cuz Iâ€™m training on my MacBook Air ğŸ˜­)*

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/image.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>Being an AI researcher in 2025</figcaption>
  </div>
</figure>


The goal of the environment will be simple:Â **reach for the target**. Sounds easy, right? But donâ€™t forgetâ€”motor skills are hard. Even humans, with their billions of neurons, struggle to master them. Just watch a toddler trying to stand up for the first time.

## Where Do We Start?

Sure, weÂ *could*Â build the environment entirely from scratch. We could create an iteration loop, design a uniform interface to switch between manual and automated control, and then implement a learning algorithm from the ground up.

But, as mama always said, â€œDonâ€™t reinvent the wheel.â€ Doing all that would be like someone asking you to make coffee, and instead of just brewing it, you fly to Ethiopia to pick the beans, dry them out, roast them, grind themâ€¦ you get the idea.

So weâ€™re gonna use some libraries to help us out.

My advice? Donâ€™t think of the following libraries as theÂ *only*Â way to do this. Theyâ€™re just here to make our lives easier. Think of them as shortcutsâ€”because who doesnâ€™t love a good shortcut?

<h3>
  Tool 1: <a href="https://gymnasium.farama.org/" target="_blank" rel="noopener noreferrer">Gymnasium</a>
</h3>

Letâ€™s start with what this tool can do for us.

Gymnasium provides a class that we can use to create custom worlds by implementing just a few functions. The best part? Once youâ€™ve built your environment, itâ€™sÂ **plug-and-play**Â with a ton of RL algorithm libraries that support Gymnasium environments.

The class weâ€™re interested in is calledÂ `Env`. It requires us to implement four main methods, and then weâ€™re good to go. Hereâ€™s what it looks like:

```python
class RobotArmEnv(gym.Env):
    def __init__(self):
        super().__init__()

    def reset(self):
        pass

    def step(self, action):
        return observation, reward, terminated, info 

    def render(self):
        pass
```

<h3>
  Tool 2: <a href="https://stable-baselines3.readthedocs.io/en/master/index.html" target="_blank" rel="noopener noreferrer">Stable Baselines 3</a>
</h3>

Next up,Â **Stable Baselines 3**. This library is a game-changerâ€”it provides prebuilt learning algorithms that work seamlessly with Gymnasium environments. Exactly what we need to get our AI up and running without starting from scratch.

Hereâ€™s how simple it is to use Stable Baselines 3:

```python
env = RobotArmEnv()
model = PPO('MlpPolicy', env, verbose=1)  # â† Your one-line trainer
model.learn(total_timesteps=1_000_000)
```

Yep, itâ€™s that straightforward. 

## Right then, time to build out the robotâ€™s world

Letâ€™s start by creating the first joint: theÂ **shoulder**. Weâ€™ll place it at the center of the screen, and itâ€™ll be able to rotate a total of 90 degrees. Attached to the shoulder is theÂ **humerus bone**Â (yes, weâ€™re borrowing human anatomy here). At the end of the humerus, weâ€™ve got theÂ **elbow joint**, which connects toâ€¦ well, where the handÂ *shouldâ€™ve*Â been. The elbow can flex up to 180 degrees, giving our bot some serious range of motion.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screenshot_2025-01-23_at_9.24.45_PM.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>Here we have a robot arm in all its glory</figcaption>
  </div>
</figure>

Now, letâ€™s spawn aÂ targetÂ at a random location within the robotâ€™s reach. But first, we need to figure out where the robot can actually reach. Hereâ€™s a plot of the regions IÂ thinkÂ the robot might be able to access with this arm setup.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screenshot_2025-01-24_at_5.00.12_PM.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
</figure>
Okay, now letâ€™s add the target to the robot arm environment:

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screenshot_2025-01-23_at_9.24.45_PM%201.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>No-frills renderer </figcaption>
  </div>
</figure>

Nahh, somethingâ€™s missing. It doesnâ€™t have that robot vibe to it.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/d9c0c834-62c6-4fea-8168-e6392b381255.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>Stylized renderer</figcaption>
  </div>
</figure>

Much better.

## Making it so we can interact with the environment

To test our newly created world, letâ€™s make it interactive by allowing control through key presses.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center', flexDirection: 'column', alignItems: 'center' }}>
    <video height="300" controls style={{ borderRadius: '8px' }}>
      <source src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screen_Recording_2025-01-24_at_3.58.38_PM.mov" type="video/mp4" />
    </video>
    <figcaption style={{ marginTop: '-20px', padding: '0', textAlign: 'center' }}>It works! But clicking buttons is tedious. Iâ€™d rather let an agent handle it.</figcaption>
  </div>
</figure>

# Agent? Like CIA Agent?

Not exactly. Here, an agent is simply a decision-making AI that interacts with its environment by taking actions.

### Letâ€™s learn by examples:

**In Fortnite**:

The agent is the character you control. The environment is the game worldâ€”the map, other players, and everything else you interact with.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screenshot_2025-01-25_at_11.54.40_AM.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
</figure>
**In a driving game**:

The agent is your car. You (or the AI) can speed up, brake, or steer. The environment is the race track, other cars, and any obstacles.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/image%201.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
</figure>

**Now in general:**

The agent interacts with the environment by taking actions which in turn might affect the environment and then it receives observations of the affected state

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/image%202.png" 
      width={300} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>Interaction Loop</figcaption>
  </div>
</figure>

### Observation

Ideally, you might want to let the robot see the environment in its entiretyâ€”raw pixels, full state, everything. But in practice, itâ€™s much more efficient to extract and provide only the most important features. For example, youÂ *could*Â teach an AI to play Pong by feeding it thousands of pixels and letting it figure out what to do. But hereâ€™s the catch: not only would the AI be learning how to play Pong, itâ€™d also be learning how toÂ *see*. And thatâ€™s a lot of extra work, making the process painfully slow.

Sure, this kind of end-to-end learning is possible, but to keep things simple, weâ€™re going to give our agent hand-crafted featuresâ€”specific pieces of information that we think will help it navigate the environment more effectively.

Hereâ€™s what our observation space looks like:

```python
def _get_obs(self):
        return np.array([
            self.a1, # Shoulder angle
            self.a2, # Elbow angle
            *self.target, # Target position
            self._calculate_distance() # Distance to target
            ])
```

Letâ€™s break this down:

1. **Shoulder angle (`self.a1`)**Â andÂ **elbow angle (`self.a2`)**:
    
    These are like the robotâ€™s sense ofÂ **proprioception**â€”the awareness of where its limbs are in space. Without this, the bot would have no idea how its arm is positioned.
    
2. **Target position (`self.target`)**:
    
    This is crucial because, well, you canâ€™t move toward something if you donâ€™t know where it is.
    
3. **Distance to target (`self._calculate_distance()`)**:
    
    Technically, the proprioception and target position should be enough. But to make things easier, weâ€™re also telling the bot how far its hand is from the target. Think of it as giving the bot a little nudge in the right direction.
    

### Actions

Just like when we controlled the bot manually using the keyboard, the AI will interact with the environment through the same set of actions:

- Increase shoulder angle
- Decrease shoulder angle
- Increase elbow angle
- Decrease elbow angle

Weâ€™ve also added one more action:Â no operationÂ (doing nothing).

## How do agents learn?

This is the heart of reinforcement learning. Unlike supervised learning, where you might explicitly tell an AI what action to take in a given state, reinforcement learning is all about letting the AI figure things out on its own. We guide the agent by rewarding good behavior and penalizing bad behavior, shaping its decisions over time.

The agent learns through a continuous loop:

1. **Observe**: The agent gathers data from the environment.
2. **Act**: The agent takes an action.
3. **Reward**: The environment provides feedback (a reward or penalty) based on the action.
4. **Learn**: The agent updates its policy to improve future decisions.

This loop is the backbone of reinforcement learning.

Here is the reward function I used to guide to robot arm:

```python
reward = (
            distance_reduction * 5.0 +      # Encourage moving closer
            proximity_reward * 0.5 +        # Incentivize proximity
            success_bonus -                 # Big reward for success
            time_penalty -                  # Discourage slow solutions
            action_penalty                  # Discourage unnecessary movements
        )
```

### Different Algorithms

While the core idea of reinforcement learning is simpleâ€”maximize rewardsâ€”there areÂ manyÂ different algorithms to achieve this. Each has its strengths and trade-offs and honestly, they deserve their own deep dive. Maybe thatâ€™s for a future blog. For the curious, I usedÂ PPO, mostly because Reddit, YouTube, and the broader RL community kept hyping it up.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/image%203.png" 
      width={550} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>The huge variety of training algorithms at our disposal</figcaption>
  </div>
</figure>

# Let the Training Runs Begin

Youâ€™d think the hard part would be setting everything upâ€”creating the environment, setting up the training loop, and all that. But no, by far the hardest part wasÂ waiting.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/WESamuu.jpg" 
      width={200} 
      style={{ borderRadius: '8px' }}
    />
  </div>
</figure>
Before diving into this robotic arm project, I messed around with a simple snake environment. That little guy mastered the game in about 10 minutes.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center', flexDirection: 'column', alignItems: 'center' }}>
    <video height="300" controls style={{ borderRadius: '8px' }}>
      <source src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screen_Recording_2025-01-25_at_7.02.16_PM.mov" type="video/mp4" />
    </video>
    <figcaption style={{ marginTop: '-20px', padding: '0', textAlign: 'center' }}>Good Snek</figcaption>
  </div>
</figure>


But this robotic arm? Oh no. This one takes its sweet, sweet time.

Hereâ€™s one of the earlier versions:

<figure>
  <div style={{ display: 'flex', justifyContent: 'center', flexDirection: 'column', alignItems: 'center' }}>
    <video height="300" controls style={{ borderRadius: '8px' }}>
      <source src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screen_Recording_2025-01-24_at_5.10.24_PM.mov" type="video/mp4" />
    </video>
    <figcaption style={{ marginTop: '-20px', padding: '0', textAlign: 'center' }}>Youâ€™ve gotta be kidding me</figcaption>
  </div>
</figure>

Remember the reward function I showed you earlier? Yeah, thatâ€™s theÂ *final*Â version I came up with after training forÂ 2 million steps. Initially, I had a much simpler reward function: the bot only got a reward when it actually hit the target. Sounds logical, right? But as it turns out, thatâ€™s not enough.

EnterÂ reward shaping: the art of designing rewards that guide the agent toward desired behavior. Instead of waiting for the bot to randomly stumble upon success, we nudge it in the right direction by rewarding smaller milestonesâ€”like moving closer to the target or reducing unnecessary movements.

Instead of starting over, I kept the same model and tweaked the reward function after 2 million steps. I was hoping some of the training would carry over. Spoiler: it did.

As the model trained, I kept an eye on two key metrics:Â mean episode lengthÂ andÂ mean total reward. The episode length dropped from 200 (the maximum steps per episode) to around 70, meaning the bot was reaching the target faster. The mean reward also crept up, albeit slowly:

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Training_Progress.png" 
      width={600} 
      style={{ borderRadius: '8px' }}
    />
  </div>

</figure>
# Watching Your AI Play

They grow up so fast ğŸ¥²:

<figure>
  <div style={{ display: 'flex', justifyContent: 'center', flexDirection: 'column', alignItems: 'center' }}>
    <video height="300" controls style={{ borderRadius: '8px' }}>
      <source src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/final_cut.mov" type="video/mp4" />
    </video>
    <figcaption style={{ marginTop: '-20px', padding: '0', textAlign: 'center' }}>Holy sh*t, it actually worked</figcaption>
  </div>
</figure>

AfterÂ 7,000,000 stepsÂ (and about 4 hours of training), our little AI has gone from shaking nervously to gracefully reaching for its target.

# Conclusion

Turns out, reinforcement learning isnâ€™t just a made-up gimmick after all. It actually works! (At least, it worked for this very simple environment that I threw together.)

But hereâ€™s a thought: my bot had to go through aroundÂ **1**00,000 gamesÂ to get to this point. Meanwhile, I bet a 10-year-old kid could learn to hit the target with their hand after playing with the controls for, like,Â 10 games.

So, why do humans learn so much faster? One theory is that we rely onÂ prior knowledgeâ€”our understanding of how the world works. We donâ€™t start from scratch every time we learn something new. Instead, we build on what we already know, which gives us a massive head start.

---

### Try It Yourself

Take this platformer, for example. 

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/image%204.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>â€œWhy canâ€™t the bot just do this in 1 try lolâ€</figcaption>
  </div>
</figure>


It feels pretty obvious what you have to do, right? Jump on the platforms, avoid the flytrap, and reach the flag. Now, imagine I flipped the game upside down. A little more difficult, but still manageable. Okay, what if I switched things up so the mushroom looks like the turtle and the turtle looks like the mushroom? Confusing, right? Now, what if I made everything completely indiscernible? Suddenly, the task becomesÂ *much*Â harder.

<figure>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <img 
      src="/blogs/training-a-robotic-arm-to-move-training-ai-in-a-custom-world/Screenshot_2025-01-25_at_6.23.46_PM.png" 
      width={350} 
      style={{ borderRadius: '8px' }}
    />
  </div>
  <div style={{ display: 'flex', justifyContent: 'center' }}>
    <figcaption>â€œWhere do I even start??â€ This is what our poor bots have to endure</figcaption>
  </div>
</figure>

 I encourage you to try this platformer game where you canâ€™t rely on your prior knowledge of the world:

ğŸ‘‰Â [Trippy Platformer](https://high-level-4.herokuapp.com/experiment)

After playing it, I empathize with my agents a lot more.

---

### Whatâ€™s Next?

If youâ€™re inspired to tinker with this project yourself, hereâ€™s the GitHub repo:

ğŸ”—Â [RL Robot Arm on GitHub](https://github.com/farrukh-saif/RL-Robot-Arm)

And if youâ€™re new to reinforcement learning, I highly recommend checking out OpenAIâ€™sÂ Spinning UpÂ guide:

ğŸ“šÂ [Spinning Up by OpenAI](https://spinningup.openai.com/en/latest/spinningup/spinningup.html)

---

### Room for Improvement

Iâ€™m fully aware that this isnâ€™t the most accurate simulation of a robotic arm. To make it more realistic, we could addÂ gravity, friction, and momentumâ€”because, well, physics is a thing. ğŸ˜…